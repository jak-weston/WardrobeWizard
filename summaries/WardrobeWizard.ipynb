{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GarmentGan Implementation\n",
    "\n",
    "## Shape transfer network\n",
    "![Shape Transfer Network Architecture](./Images/shape_transfer_network_architecture.png)\n",
    "\n",
    "### Semantic segmentation / Masking  \n",
    "* Semantic parser extracts pixel-wise semantic map of reference person into 10 categories (hat, face/hair, torso, etc.)\n",
    "* Masks smallest rectangles to target area (torso, arms, top clothes) to retain parts of reference person during transfer\n",
    "\n",
    "![Masking Input Map Hands](./Images/masking_input_map_hands.png)\n",
    "\n",
    "* Retains semantic information of left and right hands using keypoints (arms, elbows, wrists) then masks them separately\n",
    "\n",
    "## Network Architecture\n",
    "### How it works\n",
    "* Employs encoder-decoder for generator for shape map\n",
    "* Takes inputs (segmentation map, person representation, image of desired clothing)\n",
    "\n",
    "### Person representation\n",
    "* Created by concatenating feature representations of body keypoints and body shape\n",
    "\n",
    "### Encoder-Decoder\n",
    "* Standard convolutional layers conv_1-5 [3x3 kernels, 2 stride]\n",
    "* Instance normalization layers normalize feature maps\n",
    "* Leaky Relu used as non-linear function\n",
    "* 4 residual blocks (bottlenecks for encoder)\n",
    "* Decoder: conv blocks and nearest interpolation for up-sampling\n",
    "\n",
    "### Adversarial Training\n",
    "* Generator trained adversarially using PatchGAN structure\n",
    "* Loss function includes parsing loss, L1 distance between ground truth and generated map, and gradient penalty loss.\n",
    "\n",
    "## Appearance transfer Network\n",
    "![Appearance Transfer Network Architecture](./Images/appearance_tranfer_network_architecture.png)\n",
    "\n",
    "### How it works\n",
    "* Takes generated segmentation maps that target clothing and body shape as inputs and generates an image portraying the reference person wearing that target clothing\n",
    "\n",
    "### Feature processing\n",
    "* Down and up sampling feature maps conducted through 2 stride convolution\n",
    "* Instance and spectral normalization are used to stabilize\n",
    "* SPADE style normalization layer used for more accurate spatial information transfer\n",
    "\n",
    "### Discriminator\n",
    "* Utilizes multi-scale SN-PatchGan for the discriminator network to generate high-quality RGB images\n",
    "\n",
    "### Loss functions\n",
    "* Loss function (LG) includes terms for geometric alignment (LTPS), pixel-wise difference (Lperâˆ’pixel), perceptual (Lpercept), and feature matching (Lfeat) losses, along with adversarial loss\n",
    "* Geometric alignment loss (LTPS) measures the difference between warped and worn clothing items.\n",
    "* Other losses ensure the generated image matches the reference image both in appearance and structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WardrobeWizard\n",
    "\n",
    "#### Semantic segmentation map\n",
    "1. Categories (clothing items)  \n",
    "    (top, bottoms, accessories, etc.)\n",
    "\n",
    "#### TPS of clothing onto person\n",
    "1. Have to run once for each piece of clothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece285",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
