{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# demo.sh\n",
    "1. Use open pose to detect keypoints (body joints) in the input images located in the 'input' directory.  These key points are written to the 'cache/' directory\n",
    "2. run matlab scrpit1\n",
    "3. test segmentation model using test_seg.lua and latest_2.t7\n",
    "4. run matlab script2\n",
    "5. test initial language encoding model using test_lang_initial.py\n",
    "6. run lua script test_gan.lua (generates outputs)\n",
    "7. create output folder\n",
    "\n",
    "Files needed to implement:  \n",
    "[openpose.bin](https://github.com/CMU-Perceptual-Computing-Lab/openpose)  \n",
    "[script1](https://vscode.dev/github/jak-weston/WardrobeWizard/blob/main/demo.sh.ipynb#C3:L19)  \n",
    "[test_seg.lua](https://vscode.dev/github/jak-weston/WardrobeWizard/blob/main/demo.sh.ipynb#C6)  \n",
    "[script2](https://vscode.dev/github/jak-weston/WardrobeWizard/blob/main/demo.sh.ipynb#C8)  \n",
    "[test_lang_initial.py](https://vscode.dev/github/jak-weston/WardrobeWizard/blob/main/demo.sh.ipynb#C10)  \n",
    "[test_gan.lua](https://vscode.dev/github/jak-weston/WardrobeWizard/blob/main/demo.sh.ipynb#C13:L109)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "1. \n",
    "./build/examples/openpose/openpose.bin --image_dir $CURRENT_DIR/input/ --no_display --write_keypoint=$CURRENT_DIR/cache\n",
    "\n",
    "2.\n",
    "matlab -nodesktop -nojvm -r \"script1; exit\"\n",
    "\n",
    "3.\n",
    "SEG_MODEL=./latest_2.t7 th test_seg.lua\n",
    "\n",
    "4. \n",
    "matlab -nodesktop -nojvm -r \"script2; exit\"\n",
    "\n",
    "5.\n",
    "python test_lang_initial.py\n",
    "\n",
    "6.\n",
    "th test_gan.lua\n",
    "\n",
    "7. \n",
    "mkdir output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# script1\n",
    "matlab scrpt that preprocesses the input images by resizing and cropping them to focus on the largest person detected in each image, and then saves the processed images in the cache/ directory. Additionally, it creates a text file listing the names of the processed images.\n",
    "\n",
    "1. lists all the PNG images in the input/ directory.\n",
    "2. checks if there are any input images available.\n",
    "3. creates a text file named script1.txt in the cache/ directory to store the processed image names.\n",
    "4. For each input image:\n",
    "    * parsee the YAML file containing pose information generated by OpenPose located in the cache/ directory.\n",
    "    * extracts the pose coordinates of detected keypoints (body joints) using custorm function (parseYML).\n",
    "    * selects the largest person in the image based on the bounding box around the pose keypoints.\n",
    "    * calculates a transformation to resize and crop the image to a standardized size of 224x224 pixels, focusing on the selected person.\n",
    "    * applies the transformation to the input image using imtransform MATLAB function.\n",
    "    * saves the transformed image in the cache/ directory.\n",
    "    * writes the filename (without extension) of the processed image to the script1. txt file (located in cache/).\n",
    "\n",
    "## parseYML.m\n",
    "reads size information and data from a YAML file, reshapes the data into a 3D array, and returns the array with dimensions consistent with MATLAB conventions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. List all the PNG images in the input/ directory.\n",
    "fn = dir('./input/*.png');\n",
    "\n",
    "#2. Check if there are any input images available.\n",
    "assert(length(fn) > 0, 'There is no input images (png and jpg only).');\n",
    "\n",
    "#3. Create a text file named script1.txt in the cache/ directory to store the processed image names.\n",
    "f = fopen('./cache/script1.txt','w');\n",
    "\n",
    "#4. For each input image:\n",
    "for i = 1:length(fn)\n",
    "    n = fn(i).name;\n",
    "    \n",
    "    #4.1. Parse the YAML file containing pose information generated by OpenPose located in the cache/ directory.\n",
    "    yml_fn = ['./cache/' n(1:end-4) '_pose.yml'];\n",
    "    x = parseYML(yml_fn);\n",
    "    \n",
    "    #4.2. Extract the pose coordinates of detected keypoints (body joints) using custom function (parseYML).\n",
    "    xdxd = zeros(size(x,1), 4);\n",
    "    for j = 1:size(x,1)\n",
    "        y = squeeze(x(j,:,:));\n",
    "        idx = find(y(:,3) > 0.1);\n",
    "        y = y(idx, :);\n",
    "        if length(idx) > 0\n",
    "            xdxd(j,1) = min(y(:,1));\n",
    "            xdxd(j,2) = max(y(:,1));\n",
    "            xdxd(j,3) = min(y(:,2));\n",
    "            xdxd(j,4) = max(y(:,2));\n",
    "        end;\n",
    "    end;\n",
    "\n",
    "    #4.3. Select the largest person in the image based on the bounding box around the pose keypoints.\n",
    "    [~, id] = max((xdxd(:,4)-xdxd(:,3)) .* (xdxd(:,2)-xdxd(:,1)));\n",
    "\n",
    "    #4.4. Calculate a transformation to resize and crop the image to a standardized size of 224x224 pixels, focusing on the selected person.\n",
    "    xmin = xdxd(id,1);\n",
    "    xmax = xdxd(id,2);\n",
    "    ymin = xdxd(id,3);\n",
    "    ymax = xdxd(id,4);\n",
    "    win_size = 224;\n",
    "    cx = 0.5;\n",
    "    cy = 0.5;\n",
    "    ylen = 0.75/2;\n",
    "    xlen = ylen / (ymax - ymin) * (xmax - xmin);\n",
    "    target_xmin = win_size * (cx - xlen);\n",
    "    target_xmax = win_size * (cx + xlen);\n",
    "    target_ymin = win_size * (cy - ylen);\n",
    "    target_ymax = win_size * (cy + ylen);\n",
    "\n",
    "    #4.4. Apply the transformation to the input image using imtransform MATLAB function.\n",
    "    t = cp2tform([xmin,ymin;xmax,ymin;xmin,ymax;xmax,ymax],...\n",
    "        [target_xmin,target_ymin; target_xmax, target_ymin; ...\n",
    "        target_xmin, target_ymax; target_xmax, target_ymax],...\n",
    "        'nonreflective similarity');\n",
    "    im = imread(['./input/' fn(i).name]);\n",
    "    im = imtransform(im2single(im), t, 'XData', [1 win_size], ...\n",
    "        'YData', [1 win_size], 'XYScale', 1);\n",
    "\n",
    "    #4.5. Save the transformed image in the cache/ directory.\n",
    "    imwrite(im, ['./cache/' fn(i).name]);\n",
    "    \n",
    "    #4.6. Write the filename (without extension) of the processed image to the script1.txt file (located in cache/).\n",
    "    fprintf(f, '%s\\n', fn(i).name(1:end-4));\n",
    "end;\n",
    "\n",
    "#4.7. Close the script1.txt file.\n",
    "fclose(f);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function x = parseYML(yml_file_name)\n",
    "# 1. Opens the YAML file specified by yml_file_name in read mode and returns the file identifier (fid).\n",
    "fid = fopen(yml_file_name, 'r');\n",
    "\n",
    "# 2. Skips the first two lines in the YAML file. These lines are typically headers or comments and are not relevant to the data parsing.\n",
    "fgetl(fid);\n",
    "fgetl(fid);\n",
    "\n",
    "# 3. Reads the line containing the sizes information from the YAML file. This line specifies the dimensions of the data array. The %f format specifier is used to read floating-point numbers.\n",
    "s = fscanf(fid, '   sizes: [ %f, %f, %f ]');\n",
    "s = s(:)';\n",
    "assert(length(s) == 3);\n",
    "\n",
    "# 4. Skips the line containing the \"data\" tag in the YAML file.\n",
    "fgetl(fid);\n",
    "\n",
    "# 5. Reads the line containing the start of the data array.\n",
    "fscanf(fid, '   data: [ ');\n",
    "\n",
    "# 6. Reads the data elements from the YAML file. The %f format specifier is used to read floating-point numbers, and prod(s) calculates the total number of elements in the data array.\n",
    "data = fscanf(fid, '%f, ', prod(s));\n",
    "\n",
    "# 7. Reshapes the data array according to the specified dimensions (s) and reorders the dimensions to match the MATLAB convention (rows, columns, depth).\n",
    "x = reshape(data, s([3 2 1]));\n",
    "\n",
    "# 8. Permutes the dimensions of the data array to the desired order (depth, rows, columns) before returning it.\n",
    "x = permute(x, [3 2 1]);\n",
    "\n",
    "# 9. Closes the file.\n",
    "fclose(fid);\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test_seg.lua\n",
    "This script  takes the preprocessed images generated by script1, passes them through the pre-trained segmentation model, and saves the segmentation results as a MAT file.\n",
    "1. loads required libraries/modules: cunn, cudnn, image, and matio (MATLAB input/output library).\n",
    "2. reads the list of processed image names from the script1.txt file located in the ./cache/ directory.\n",
    "3. loads a pre-trained segmentation model from the path specified in the SEG_MODEL environment variable.\n",
    "4. initializes a tensor input to hold the processed images.\n",
    "5. iterates over each image:\n",
    "    * Loads the image using image.load.\n",
    "    * Converts the image to a FloatTensor and assigns it to the corresponding index in the input tensor.\n",
    "6. moves the input tensor to the GPU (if available).\n",
    "7. passes the input tensor through the segmentation model to obtain the output.\n",
    "8. applies softmax to the output tensor to get class probabilities.\n",
    "9. saves the class probabilities to a MATLAB MAT file named test_seg.mat in the ./cache/ directory.\n",
    "\n",
    "Look into;  \n",
    "SEG_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load required Torch modules\n",
    "require 'cunn'\n",
    "require 'cudnn'\n",
    "require 'image'\n",
    "local matio = require 'matio'\n",
    "\n",
    "# 2. Read the list of processed image names from the script1.txt file\n",
    "local namesFile = io.open('./cache/script1.txt')\n",
    "local idx = 1\n",
    "local nameList = {}\n",
    "for line in namesFile:lines() do\n",
    "    nameList[idx] = line\n",
    "    idx = idx + 1\n",
    "end\n",
    "namesFile:close()\n",
    "\n",
    "# 3. Load a pre-trained segmentation model\n",
    "local model = torch.load(os.getenv('SEG_MODEL'))\n",
    "\n",
    "# 4. Initialize a tensor to hold the processed images\n",
    "local win_size = 224\n",
    "local input = torch.zeros(idx - 1, 3, win_size, win_size):float()\n",
    "\n",
    "# 5. Iterate over each image:\n",
    "for i = 1, idx - 1 do\n",
    "    # Load the image using image.load\n",
    "    local im = image.load('./cache/' .. nameList[i] .. '.png', 3, 'float')\n",
    "    \n",
    "    # Convert the image to a FloatTensor and assign it to the corresponding index in the input tensor\n",
    "    input[{{i},{},{},{}}] = im\n",
    "end\n",
    "\n",
    "# 6. Move the input tensor to the GPU (if available)\n",
    "input = input:cuda()\n",
    "\n",
    "# 7. Pass the input tensor through the segmentation model to obtain the output\n",
    "local output = model:forward(input)\n",
    "\n",
    "# 8. Apply softmax to the output tensor to get class probabilities\n",
    "local prob = cudnn.SpatialSoftMax():cuda():forward(output):float()\n",
    "\n",
    "# 9. Save the class probabilities to a MATLAB MAT file\n",
    "matio.save('./cache/test_seg.mat', {prob=prob})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# script2\n",
    "1. loads preporcessed image names from script1.txt and segmentation probabilities from test_seg.mat\n",
    "2. rearranges the segmentation probabilites tensor (prob) to the correct dimensions\n",
    "3. defines parameters window size (win_size) and the number of classes (L)\n",
    "4. Loops through each image and preforms post-processing on segmentation results\n",
    "    * Utilizes the DenseCRF algorithm (DCRF) to refine the segmentation results (bb) based on the input image and probabilities.\n",
    "    * maps the refined segmentation results to a predefined set of labels (label_assign).\n",
    "    * resizes and prepares the segmentation results for further processing.\n",
    "5. Language Preprocessing\n",
    "    * extracts and preprocesses text descriptions associated with each image\n",
    "    * maps words in descriptions to numberical embeddings using the predefined \"map\"\n",
    "6. saves processed segmentation results (seg_final), language embeddings (codej) and resized segmentation maps (b_) to a MATLAB MAT file script2.mat\n",
    "\n",
    "Look into:\n",
    "DCRF.mex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear;\n",
    "addpath('./mex');  # 1. Add mex directory to the MATLAB search path\n",
    "load map map;  # 2. Load the 'map' variable from a file named 'map' into the workspace\n",
    "\n",
    "f = fopen('./cache/script1.txt','r');  # 1. Open the file 'script1.txt' for reading\n",
    "load ./cache/test_seg.mat prob;  # 1. Load the 'prob' variable from a MAT file into the workspace\n",
    "prob = permute(prob, [3,4,2,1]);  # 2. Permute the dimensions of 'prob' array\n",
    "\n",
    "win_size = 224;  # 3. Set the window size\n",
    "L = 18;  # 3. Set the number of classes\n",
    "i = 0;  # Initialize counter variable\n",
    "\n",
    "label_assign = [1,1,2,3,3,   3,3,3,3,3,   2,3,3,3,3,   3,3,4];  # Define label assignments\n",
    "lrc = cell(10000,1);  # Initialize cell array to store results\n",
    "codeJ = cell(10000,1);  # Initialize cell array to store language embeddings\n",
    "seg_final = cell(10000,1);  # Initialize cell array to store segmentation results\n",
    "\n",
    "# 4. Loop through each line in the file 'script1.txt'\n",
    "while ~feof(f)\n",
    "    fn = fgetl(f);  # 4. Read a line from the file\n",
    "    i = i + 1;  # Increment counter\n",
    "\n",
    "    img = imread(['./cache/' fn '.png']);  # Read the corresponding image file\n",
    "    bb = DCRF(img, prob(:,:,:,i), L, [3 3 5 30 30 10 10 10 9 5]);  # Utilize DenseCRF for segmentation refinement\n",
    "\n",
    "    bb = bb + 1;  # Increment segmentation results\n",
    "    seg_final{i} = bb;  # Store refined segmentation results\n",
    "\n",
    "    # 4. Apply label assignment to segmentation results\n",
    "    b_temp = bb;\n",
    "    for j = 1:L\n",
    "        b_temp(bb == j) = label_assign(j);\n",
    "    end;\n",
    "\n",
    "    # 4. Resize and prepare segmentation results\n",
    "    lrc{i} = zeros([8,8,4]);\n",
    "    for j = 1:4\n",
    "        lrc{i}(:,:,j) = imresize(single(b_temp == j), [8,8]);\n",
    "    end;\n",
    "\n",
    "    # 5. Extract and preprocess text descriptions\n",
    "    fid = fopen(['./input/' fn '.txt'], 'r');\n",
    "    s = fgetl(fid);\n",
    "    fclose(fid);\n",
    "    s = strtrim(s);\n",
    "    while(s(end) == '.')\n",
    "        s = s(1:end-1);\n",
    "    end;\n",
    "    ss = strsplit(s, ' ');\n",
    "    for j = 1:length(ss)\n",
    "        codeJ{i} = [codeJ{i} double(map(ss{j}))];  # Map words to numerical embeddings\n",
    "    end;\n",
    "    codeJ{i} = codeJ{i}(:);  # Reshape language embeddings\n",
    "end;\n",
    "\n",
    "fclose(f);  # Close the file 'script1.txt'\n",
    "lrc = lrc(1:i);  # Trim excess cells from lrc\n",
    "codeJ = codeJ(1:i);  # Trim excess cells from codeJ\n",
    "\n",
    "lr = zeros(8,8,4,i);  # Initialize array for storing lrc results\n",
    "for j = 1:i\n",
    "    lr(:,:,:,j) = lrc{j};  # Populate lr array with lrc results\n",
    "end;\n",
    "\n",
    "seg_final = seg_final(1:i);  # Trim excess cells from seg_final\n",
    "\n",
    "fine_size = 128;  # Set fine size\n",
    "b_ = zeros(fine_size, fine_size, 1, i);  # Initialize array for resized segmentation maps\n",
    "label_assign = [1,1,2,3,4, 4,4,4,5,5, 2,5,5,6,6, 4,3,0];  # Define label assignments for resizing\n",
    "\n",
    "# 4. Resize segmentation maps and store in b_\n",
    "for j = 1:i\n",
    "    t = seg_final{j};\n",
    "    v = t;\n",
    "    for k = 1:L\n",
    "        v(t == k) = label_assign(k);\n",
    "    end;\n",
    "    b_(:,:,:,j) = imresize(v, [fine_size, fine_size], 'nearest');\n",
    "end;\n",
    "\n",
    "# 6. Save processed data to a MATLAB MAT file\n",
    "save('./cache/script2.mat', 'lr', 'codeJ', 'b_', 'seg_final');\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test_lang_initial.py\n",
    "The neural network consists of an RNN layer followed by several linear layers. The RNN layer processes sequential input data (language embeddings) and generates hidden states. The linear layers take the last hidden state from the RNN and produce predictions for different attributes (category, color, gender, sleeve).\n",
    "1. implement the __init__ method\n",
    "    * Use nn.RNN for the RNN layer and nn.Linear for the linear layers.\n",
    "    * Define the dimensions for the input vocabulary (dim_voc), hidden state (dim_h), and other output dimensions (dim_cate_new, dim_color, dim_gender, dim_sleeve).\n",
    "    * Specify the number of RNN layers (num_layers).\n",
    "2. implement the forward method\n",
    "    * Initialize the hidden state (h0) with zeros using torch.zeros.\n",
    "    * Pass the input tensor (x) through the RNN layer using self.rnn.\n",
    "    * Extract the last hidden state (hn2) from the RNN output.\n",
    "    * Pass hn2 through the linear layers (net_cate_new, net_color, net_gender, net_sleeve) to get predictions for different attributes.\n",
    "    * Return the final hidden state (hn2) along with the predictions.\n",
    "3. Create Model Instance\n",
    "    * Create an instance of the define_network class.\n",
    "    * Move the model to the GPU using .cuda() to utilize GPU acceleration.\n",
    "    * Load pre-trained weights into the model using model.load_state_dict(torch.load('rnn_latest.pth')).\n",
    "    * Set the model to evaluation mode using model.eval().\n",
    "4. Pass Input Through the Model:\n",
    "    * Convert the input data (language embeddings) to the appropriate format (one-hot encoding).\n",
    "    * Pass the input through the model by calling the model instance with the input tensor as an argument.\n",
    "    * Obtain the final hidden state and predictions from the model output.\n",
    "5. Save the computed final hidden states to a MATLAB MAT file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from scipy.io import loadmat, savemat\n",
    "\n",
    "# Load preprocessed language embeddings from script2.mat\n",
    "mat = loadmat('./cache/script2.mat')\n",
    "codeJ = mat['codeJ']\n",
    "\n",
    "# Define parameters and dimensions for the neural network\n",
    "dim_voc = 539  # Dimension of the vocabulary\n",
    "bsz = 1  # Batch size\n",
    "dim_h = 100  # Dimension of the hidden state\n",
    "dim_cate_new = 19  # Dimension of category new\n",
    "dim_color = 17  # Dimension of color\n",
    "dim_gender = 2  # Dimension of gender\n",
    "dim_sleeve = 4  # Dimension of sleeve\n",
    "num_layers = 2  # Number of RNN layers\n",
    "\n",
    "# Define the neural network architecture\n",
    "class define_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(define_network, self).__init__()\n",
    "        # 1. Implement the __init__ method\n",
    "        self.rnn = nn.RNN(dim_voc, dim_h, num_layers)\n",
    "        self.net_cate_new = nn.Linear(dim_h, dim_cate_new)\n",
    "        self.net_color = nn.Linear(dim_h, dim_color)\n",
    "        self.net_gender = nn.Linear(dim_h, dim_gender)\n",
    "        self.net_sleeve = nn.Linear(dim_h, dim_sleeve)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 2. Implement the forward method\n",
    "        # Initialize hidden state\n",
    "        h0 = Variable(torch.zeros(num_layers, bsz, dim_h).cuda())\n",
    "        # Forward pass through the RNN\n",
    "        _, hn = self.rnn(x, h0)\n",
    "        # Extract the last hidden state\n",
    "        hn2 = hn[-1]\n",
    "        # Pass the hidden state through linear layers to get predictions\n",
    "        y_cate_new = self.net_cate_new(hn2)\n",
    "        y_color = self.net_color(hn2)\n",
    "        y_gender = self.net_gender(hn2)\n",
    "        y_sleeve = self.net_sleeve(hn2)\n",
    "        return hn2, y_cate_new, y_color, y_gender, y_sleeve\n",
    "\n",
    "# Create an instance of the neural network, move it to the GPU, and load pre-trained weights\n",
    "model = define_network()\n",
    "model.cuda()\n",
    "model.load_state_dict(torch.load('rnn_latest.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Initialize an array to store the final hidden states for each sample\n",
    "test_hn2 = np.zeros((len(codeJ), dim_h))\n",
    "\n",
    "# Iterate over each sample in the codeJ array\n",
    "for sample_id in range(len(codeJ)):\n",
    "    c = codeJ[sample_id][0]\n",
    "    l = len(c)\n",
    "    # Initialize a tensor to store one-hot encoded input\n",
    "    cuda_c_onehot = torch.zeros(l, bsz, dim_voc).cuda()\n",
    "    \n",
    "    # Convert each codeJ sample to one-hot encoding\n",
    "    for i in range(l):\n",
    "        cuda_c_onehot[i][0][int(c[i][0]-1)] = 1\n",
    "    \n",
    "    # Wrap the one-hot encoding in a Variable and pass it through the neural network\n",
    "    cuda_c_onehot_v = Variable(cuda_c_onehot)\n",
    "    hn2, _, _, _, _ = model(cuda_c_onehot_v)\n",
    "    \n",
    "    # Store the final hidden state in the test_hn2 array\n",
    "    test_hn2[sample_id] = hn2.data[0].cpu().numpy()\n",
    "\n",
    "# Save the computed final hidden states to a MATLAB MAT file\n",
    "result = {\"hn2\": test_hn2}\n",
    "savemat(\"./cache/test_lang_initial.mat\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test_gan.lua\n",
    "implement a neural network-based image synthesis process.\n",
    "\n",
    "1. Load Configuration: \n",
    "    * Initialize the testConf table with various configuration (number of input, output channels, window sizes, and the number of conditions)\n",
    "2. Load Data: Load data from MAT files and perform necessary operations:\n",
    "    * Load segmentation results (lr) and language embeddings (text) from MAT files.\n",
    "    * Load ih_mean matrix.\n",
    "    * Load test_set_b_ array.\n",
    "    * Load pre-trained models (G1, G2) from files (sr1.t7, ih1_skip.t7).\n",
    "3. Preprocess Data:\n",
    "    * Permute dimensions and ensure data is contiguous.\n",
    "    * Normalize data and create noise (z).\n",
    "4. Forward Pass Through Model:\n",
    "    * Pass the input data (z, text, lr) through the pre-trained models (G1, G2).\n",
    "    * Apply convolutions and other operations to generate the synthesized image.\n",
    "5. Postprocess and Save Results:\n",
    "    * Postprocess the output to obtain the final synthesized image.\n",
    "    * Save the synthesized images to files.\n",
    "\n",
    "files used in the Lua script:\n",
    "'./sr1.t7'  \n",
    "'./ih1_skip.t7'  \n",
    "'./ih_mean.mat'  \n",
    "'./cache/script2.mat'  \n",
    "'./cache/test_lang_initial.mat'  \n",
    "'./cache/script1.txt'  \n",
    "'./cache/' .. line .. '.png' (where line is a filename read from 'script1.txt')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "require 'nngraph'\n",
    "require 'cunn'\n",
    "require 'cudnn'\n",
    "require 'image'\n",
    "local matio = require 'matio'\n",
    "\n",
    "local testConf = {}\n",
    "testConf.n_map_all = 7\n",
    "testConf.n_z = 80\n",
    "testConf.nt_input = 100\n",
    "testConf.n_condition = 4\n",
    "testConf.disp_win_id = 50\n",
    "testConf.win_size = 128\n",
    "testConf.lr_win_size = 8\n",
    "testConf.nc = 3\n",
    "testConf.n_condition_2 = 3\n",
    "\n",
    "local lr = matio.load('./cache/script2.mat','lr')  # 2. Load segmentation results\n",
    "lr = lr:permute(4,3,1,2)  # 2. Permute dimensions\n",
    "lr = lr:contiguous()  # 2. Ensure data is contiguous\n",
    "local m = lr:size(1)  # Get size of lr\n",
    "local text = matio.load('./cache/test_lang_initial.mat', 'hn2')  # 2. Load language embeddings\n",
    "text = text:contiguous()  # 2. Ensure data is contiguous\n",
    "text = text:view(m, testConf.nt_input, 1, 1)  # 2. Reshape text\n",
    "local ih_mean_temp = matio.load('./ih_mean.mat', 'ih_mean')  # 2. Load ih_mean matrix\n",
    "local ih_mean = ih_mean_temp:permute(3,1,2):contiguous():view(1,testConf.nc,testConf.win_size,testConf.win_size)  # 2. Permute dimensions and reshape ih_mean\n",
    "local test_set_b_ = matio.load('./cache/script2.mat', 'b_')  # 2. Load test_set_b_\n",
    "test_set_b_ = test_set_b_:permute(4,3,1,2)  # 2. Permute dimensions\n",
    "test_set_b_:contiguous()  # 2. Ensure data is contiguous\n",
    "\n",
    "local a = torch.load('./sr1.t7')  # 2. Load pre-trained model G1\n",
    "local G1 = a.G\n",
    "a = nil\n",
    "local b = torch.load('./ih1_skip.t7')  # 2. Load pre-trained model G2\n",
    "local G2 = b.G\n",
    "b = nil\n",
    "\n",
    "lr = lr:cuda()  # 3. Move lr to GPU\n",
    "text = text:cuda()  # 3. Move text to GPU\n",
    "local z = torch.Tensor(m, testConf.n_z, 1,1)  # Initialize noise\n",
    "z = z:cuda()  # 3. Move noise to GPU\n",
    "z:normal(0,1)  # 3. Generate random normal noise\n",
    "local out1 = G1:forward{z, text, lr}:float()  # 4. Forward pass through G1\n",
    "local kernel = image.gaussian(5, 3):float():contiguous()  # 4. Create Gaussian kernel\n",
    "for i = 1,m do  # 4. Loop through each sample\n",
    "    for j = 1, testConf.n_map_all do  # 4. Loop through each segmentation map\n",
    "        out1[{{i},{j},{},{}}] = image.convolve(out1[{{i},{j},{},{}}]:view(testConf.win_size, testConf.win_size), kernel, 'same'):contiguous():view(1,1,testConf.win_size, testConf.win_size)  # 4. Smooth the output of G1\n",
    "    end\n",
    "end\n",
    "_, out1pmax = torch.max(out1, 2)  # 4. Get the indices of maximum values along the second dimension\n",
    "out1pmax[out1pmax:eq(testConf.n_map_all)] = 0  # 4. Replace the maximum index if it's equal to n_map_all\n",
    "\n",
    "local cb = {torch.Tensor{3,2,1,1,2,3,2}, torch.Tensor{2,3,3,2,1,1,2}, torch.Tensor{1,1,2,3,3,2,2}}  # 4. Define color balance coefficients\n",
    "for i = 1,testConf.n_condition_2 do cb[i] = cb[i] * 0.25 end  # 4. Adjust color balance coefficients\n",
    "local H = torch.Tensor{0.0030,0.0133,0.0219,0.0133,0.0030,0.0133,0.0596,0.0983,0.0596,0.0133,0.0219,0.0983,0.1621,0.0983,0.0219,0.0133,0.0596,0.0983,0.0596,0.0133,0.0030,0.0133,0.0219,0.0133,0.0030}:view(5,5):float()  # 4. Define Gaussian blur kernel\n",
    "\n",
    "local batch_condition = torch.Tensor(m, testConf.n_condition_2, testConf.win_size, testConf.win_size)  # 4. Initialize tensor for batch conditions\n",
    "for i = 1,m do  # 4. Loop through each sample\n",
    "    local t = out1pmax[{{i},{1},{},{}}]  # 4. Get the segmentation map\n",
    "    for j = 1,testConf.n_condition_2 do  # 4. Loop through each condition\n",
    "        local u = torch.Tensor(1,1,testConf.win_size, testConf.win_size)  # 4. Initialize temporary tensor\n",
    "        for k = 1,testConf.n_map_all do  # 4. Loop through each class\n",
    "            u[t:eq(k%testConf.n_map_all)] = cb[j][k]  # 4. Apply color balance\n",
    "        end\n",
    "        local v = image.convolve(u:squeeze():float(), H:float(), 'same'):contiguous()  # 4. Apply Gaussian blur\n",
    "        batch_condition[{{i},{j},{},{}}] = v:view(1,1,testConf.win_size, testConf.win_size)  # 4. Store the conditioned result\n",
    "    end\n",
    "end\n",
    "batch_condition = batch_condition - 0.5  # 4. Adjust batch condition\n",
    "\n",
    "z:normal(0,1)  # 5. Generate random normal noise\n",
    "batch_condition = batch_condition:cuda()  # 5. Move batch condition to GPU\n",
    "local out2 = G2:forward{z, text, batch_condition}:float()  # 5. Forward pass through G2\n",
    "\n",
    "local namesFile = io.open('./cache/script1.txt')  # 6. Open script1.txt\n",
    "local test_set_ih = torch.zeros(m, testConf.nc, testConf.win_size, testConf.win_size):float()  # 6. Initialize tensor for test_set_ih\n",
    "local nameList = {}  # 6. Initialize list for names\n",
    "local idx = 0  # 6. Initialize index\n",
    "for line in namesFile:lines() do  # 6. Iterate over each line in script1.txt\n",
    "    idx = idx + 1  # 6. Increment index\n",
    "    test_set_ih[{{idx},{},{},{}}] = image.scale(image.load('./cache/' .. line .. '.png', 3, 'float'), testConf.win_size, testConf.win_size)  # 6. Load and resize images\n",
    "    nameList[idx] = line  # 6. Store the filename\n",
    "end\n",
    "namesFile:close()  # 6. Close script1.txt\n",
    "\n",
    "local out2_final = out2:clone()  # 6. Clone the output of G2\n",
    "for i = 1,m do  # 6. Loop through each sample\n",
    "    for j = 1, testConf.nc do  # 6. Loop through each channel\n",
    "        local t = out2[{{i},{j},{},{}}] + ih_mean[{{},{j},{},{}}]  # 6. Add ih_mean\n",
    "        local s = test_set_ih[{{i},{j},{},{}}]  # 6. Get test_set_ih\n",
    "        local ori_b_ = test_set_b_[{{i},{},{},{}}]  # 6. Get original segmentation map\n",
    "        local now_b_ = out1pmax[{{i},{},{},{}}]  # 6. Get new segmentation map\n",
    "        t[ori_b_:eq(1)] = s[ori_b_:eq(1)]  # 6. Replace background pixels\n",
    "        t[ori_b_:eq(2)] = s[ori_b_:eq(2)]  # 6. Replace foreground pixels\n",
    "        if ori_b_:eq(5):sum() > 0 and now_b_:eq(5):sum() > 0 then  # 6. Check if class 5 exists\n",
    "            local sc5 = s[ori_b_:eq(5)]:median()  # 6. Compute median of class 5\n",
    "            t[now_b_:eq(5)] = (t[now_b_:eq(5)] + sc5[1]) / 2  # 6. Adjust class 5 pixels\n",
    "        end\n",
    "        if ori_b_:eq(6):sum() > 0 and now_b_:eq(6):sum() > 0 then  # 6. Check if class 6 exists\n",
    "            local sc6 = s[ori_b_:eq(6)]:median()  # 6. Compute median of class 6\n",
    "            t[now_b_:eq(6)] = (t[now_b_:eq(6)] + sc6[1]) / 2  # 6. Adjust class 6 pixels\n",
    "        end\n",
    "        out2_final[{{i},{j},{},{}}] = t  # 6. Store the final output\n",
    "    end\n",
    "end\n",
    "\n",
    "for i = 1, m do  # 6. Loop through each sample\n",
    "    image.save('./output/' .. nameList[i] .. '.png', out2_final[{{i},{},{},{}}]:view(testConf.nc, testConf.win_size, testConf.win_size))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# net_graph_st1.lua\n",
    "Gshape:\n",
    "\n",
    "Starts from local input_data = nn.Identity()() to local g_extra = g1 - deconv(ngf*16, ngf*8, 4,4,2,2,1,1) - bn4(ngf*8) - relu(inplace)\n",
    "This part processes the original segmentation map and the encoded text to generate an intermediate feature map.\n",
    "Gimage:\n",
    "\n",
    "Starts from local input_condition = nn.Identity()() to local g5 = g4 - deconv(ngf, nc, 4,4,2,2,1,1)\n",
    "This part processes the intermediate feature map and the condition input to generate the final output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "require 'nngraph'\n",
    "\n",
    "# Function to initialize weights\n",
    "local function weights_init(m)\n",
    "   local name = torch.type(m)\n",
    "   if name:find('Convolution') then\n",
    "      m.weight:normal(0.0, 0.02 / 16)\n",
    "      m:noBias()\n",
    "   elseif name:find('BatchNormalization') then\n",
    "      if m.weight then m.weight:normal(1.0, 0.02) end\n",
    "      if m.bias then m.bias:fill(0) end\n",
    "   end\n",
    "end\n",
    "\n",
    "# Load configuration\n",
    "local config = dofile('./config_sr1.lua')\n",
    "\n",
    "local nc = config.n_map_all\n",
    "local ncondition = config.n_condition\n",
    "\n",
    "local nz = config.nz\n",
    "local nt_input = config.nt_input\n",
    "local nt = config.nt\n",
    "\n",
    "local ndf = 64\n",
    "local ngf = 64\n",
    "local inplace = true\n",
    "\n",
    "local bn4 = nn.SpatialBatchNormalization\n",
    "local conv = nn.SpatialConvolution\n",
    "local deconv = nn.SpatialFullConvolution\n",
    "local relu = nn.ReLU\n",
    "local lerelu = nn.LeakyReLU\n",
    "\n",
    "# ##########################\n",
    "# # Gshape: Part of netG that handles segmentation generation #\n",
    "# ##########################\n",
    "\n",
    "local input_data = nn.Identity()()  # Input: Original segmentation map S0\n",
    "local input_encode = nn.Identity()()  # Input: Encoded text description d\n",
    "\n",
    "# Encode text description\n",
    "local h1 = input_encode - conv(nt_input, nt, 1, 1) - lerelu(0.2, inplace)\n",
    "\n",
    "# Concatenate input data and encoded text\n",
    "local input_data_encode = nn.JoinTable(2)({input_data, h1})\n",
    "\n",
    "# Initial layers of the generator to generate segmentation map\n",
    "local g1 = input_data_encode - deconv(nz+nt, ngf*16, 4,4) - bn4(ngf*16) - relu(inplace)\n",
    "local g_extra = g1 - deconv(ngf*16, ngf*8, 4,4,2,2,1,1) - bn4(ngf*8) - relu(inplace)\n",
    "\n",
    "# ##########################\n",
    "# # Gimage: Part of netG that handles image generation #\n",
    "# ##########################\n",
    "\n",
    "local input_condition = nn.Identity()()  # Input: Condition (e.g., low-res image or additional attributes)\n",
    "\n",
    "# Process condition input\n",
    "local f1 = input_condition - conv(ncondition, ngf, 3,3,1,1,1,1) - bn4(ngf) - lerelu(0.2, inplace)\n",
    "local f_extra = f1 - conv(ngf, ngf*2, 3,3,1,1,1,1) - bn4(ngf*2) - lerelu(0.2, inplace)\n",
    "\n",
    "# Join processed inputs\n",
    "local g2 = nn.JoinTable(2)({g_extra, f_extra}) - deconv(ngf*10, ngf*4, 4,4,2,2,1,1) - bn4(ngf*4) - relu(inplace)\n",
    "\n",
    "# Additional convolutional and deconvolutional layers for image synthesis\n",
    "local m1 = g2 - conv(ngf*4, ngf*8, 3,3,1,1,1,1) - bn4(ngf*8) - lerelu(0.2, inplace)\n",
    "local m2 = m1 - conv(ngf*8, ngf*8, 3,3,1,1,1,1) - bn4(ngf*8) - lerelu(0.2, inplace)\n",
    "local m3 = m2 - conv(ngf*8, ngf*4, 3,3,1,1,1,1) - bn4(ngf*4) - lerelu(0.2, inplace)\n",
    "\n",
    "local g3 = m3 - deconv(ngf*4, ngf*2, 4,4,2,2,1,1) - bn4(ngf*2) - relu(inplace)\n",
    "local g4 = g3 - deconv(ngf*2, ngf*1, 4,4,2,2,1,1) - bn4(ngf*1) - relu(inplace)\n",
    "local g5 = g4 - deconv(ngf, nc, 4,4,2,2,1,1)\n",
    "\n",
    "# Combine the generator network\n",
    "local netG = nn.gModule({input_data, input_encode, input_condition},{g5})\n",
    "netG:apply(weights_init)\n",
    "\n",
    "# ##########################\n",
    "# # Discriminator Network (netD) #\n",
    "# ##########################\n",
    "\n",
    "local output_data = nn.Identity()()\n",
    "local output_data_softmax = output_data - cudnn.SpatialSoftMax()\n",
    "\n",
    "local d1 = output_data_softmax - conv(nc, ndf, 4, 4, 2, 2, 1, 1) - lerelu(0.2, inplace)\n",
    "local d2 = d1 - conv(ndf, ndf*2, 4, 4, 2, 2, 1, 1) - bn4(ndf*2) - lerelu(0.2, inplace)\n",
    "local d3 = d2 - conv(ndf*2, ndf*4, 4, 4, 2, 2, 1, 1) - bn4(ndf*4) - lerelu(0.2, inplace)\n",
    "\n",
    "local mid1 = d3 - conv(ndf*4, ndf*8, 3, 3, 1, 1, 1, 1) - bn4(ndf*8) - lerelu(0.2, inplace)\n",
    "local mid2 = mid1 - conv(ndf*8, ndf*8, 3, 3, 1, 1, 1, 1) - bn4(ndf*8) - lerelu(0.2, inplace)\n",
    "local mid3 = mid2 - conv(ndf*8, ndf*4, 3, 3, 1, 1, 1, 1) - bn4(ndf*4) - lerelu(0.2, inplace)\n",
    "\n",
    "local d4 = mid3 - conv(ndf*4, ndf*8, 4, 4, 2, 2, 1, 1) - bn4(ndf*8) - lerelu(0.2, inplace)\n",
    "\n",
    "local output_condition = nn.Identity()()\n",
    "local c1 = output_condition - conv(ncondition, ndf, 3, 3, 1, 1, 1, 1) - lerelu(0.2, inplace)\n",
    "local c2 = c1 - conv(ndf, ndf*2, 3, 3, 1, 1, 1, 1) - bn4(ndf*2) - lerelu(0.2, inplace)\n",
    "\n",
    "local d_extra = nn.JoinTable(2)({d4, c2}) - conv(ndf*10, ndf*8, 4, 4, 2, 2, 1, 1) - bn4(ndf*8) - lerelu(0.2, inplace)\n",
    "\n",
    "# ##########################\n",
    "# # Discriminator Auxiliary Network #\n",
    "# ##########################\n",
    "\n",
    "local output_encode = nn.Identity()()\n",
    "local b1 = output_encode - conv(nt_input, nt, 1, 1) - bn4(nt) - lerelu(0.2, inplace) - nn.Replicate(4, 3) - nn.Replicate(4, 4)\n",
    "local d_extra_b1 = nn.JoinTable(2)({d_extra, b1}) - conv(ndf*8 + nt, ndf*8, 1, 1) - bn4(ndf*8) - lerelu(0.2, inplace)\n",
    "\n",
    "local d5 = d_extra_b1 - conv(ndf*8, 1, 4, 4) - nn.Sigmoid()\n",
    "\n",
    "# Combine the discriminator network\n",
    "local netD = nn.gModule({output_data, output_encode, output_condition},{d5})\n",
    "netD:apply(weights_init)\n",
    "\n",
    "return netG, netD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_lua\n",
    "the implementation of the first stage of FashionGAN, which involves generating human segmentation maps based on the input images and textual descriptions. The generated segmentation maps serve as the basis for the subsequent stage of texture rendering to produce the final synthesized images.\n",
    "\n",
    "1. Initialization and Setup: The code initializes necessary libraries, loads configuration files, and sets up the required environment for training the GAN.\n",
    "\n",
    "2. Data Preparation: The code loads data, including low-resolution images (lr), text encodings (text), and segmentation maps (b_). It preprocesses the data for training.\n",
    "\n",
    "3. Model Setup: The GAN architecture for generating the segmentation map (Gshape) is defined and loaded. The Discriminator network (D) is also set up.\n",
    "\n",
    "4. Training Loop: The code contains a training loop where the Generator (Gshape) and Discriminator (D) are trained iteratively. The training process involves optimizing the parameters of both networks using the Adam optimizer.\n",
    "\n",
    "5. Loss Calculation: Within the training loop, the code computes the adversarial loss for the Discriminator (errD) and the Generator (errG). Additionally, it computes the segmentation loss (errSeg) to ensure that the generated segmentation map matches the desired output.\n",
    "\n",
    "6. Visualization and Logging: The code periodically displays the training progress and visualizes the generated segmentation maps.\n",
    "\n",
    "7. Model Saving: The trained models are saved periodically for later use or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "require 'nngraph' # Load the nngraph library for building neural networks\n",
    "require 'cunn' # Load the cunn library for CUDA (GPU) support\n",
    "require 'cudnn' # Load the cudnn library for CUDA deep neural networks\n",
    "require 'hdf5' # Load the hdf5 library for handling HDF5 files\n",
    "require 'image' # Load the image library for image processing\n",
    "require 'optim' # Load the optim library for optimization algorithms\n",
    "require 'paths' # Load the paths library for file system operations\n",
    "local vis = dofile('../codes_lua/vis.lua') # Load a custom visualization script\n",
    "local matio = require 'matio' # Load the matio library for MATLAB file format support\n",
    "\n",
    "train_ind = matio.load('../data_release/benchmark/ind.mat','train_ind'):view(-1) # Load and reshape training indices\n",
    "\n",
    "local dispSurrogate = dofile('../codes_lua/dispSurrogate.lua') # Load a custom display script\n",
    "local disp = require 'display' # Load the display library for visualizations\n",
    "local getNet = dofile('../codes_lua/getNet.lua') # Load a custom network setup script\n",
    "torch.setdefaulttensortype('torch.FloatTensor') # Set the default tensor type to float\n",
    "\n",
    "local theme = 'sr1' # Define the theme or experiment name\n",
    "assert(theme == 'sr1') # Ensure the theme is correctly set\n",
    "\n",
    "local config = dofile('./config_sr1.lua') # Load the configuration file\n",
    "local G, D\n",
    "G, D = dofile('./net_graph_sr1.lua') # Load the network architectures for Generator (G) and Discriminator (D)\n",
    "\n",
    "local h5file = hdf5.open('../data_release/supervision_signals/G1.h5', 'r') # Open an HDF5 file for reading\n",
    "local b_ = h5file:read('/b_'):all() # Read the dataset\n",
    "h5file:close() # Close the HDF5 file\n",
    "local n_file = b_:size(1) # Get the number of files\n",
    "\n",
    "local lr = matio.load('../data_release/test_phase_inputs/sr1_8.mat','d') # Load low-resolution images\n",
    "lr = lr:permute(4,3,1,2) # Permute dimensions of the low-resolution images\n",
    "\n",
    "local text = matio.load('../data_release/test_phase_inputs/encode_hn2_rnn_100_2_full.mat', 'hn2') # Load text encodings\n",
    "text = text:contiguous() # Ensure the text tensor is contiguous in memory\n",
    "\n",
    "config.lr = 0.0002 # Set the learning rate\n",
    "config.beta1 = 0.5 # Set the beta1 parameter for Adam optimizer\n",
    "\n",
    "local criterion = nn.BCECriterion() # Binary Cross Entropy loss for GAN\n",
    "local cri_seg = cudnn.SpatialCrossEntropyCriterion() # Cross Entropy loss for segmentation\n",
    "local optimStateG = { # Optimization state for Generator\n",
    "    learningRate = config.lr,\n",
    "    beta1 = config.beta1,\n",
    "}\n",
    "local optimStateD = { # Optimization state for Discriminator\n",
    "    learningRate = config.lr,\n",
    "    beta1 = config.beta1,\n",
    "}\n",
    "\n",
    "local nz = config.nz # Latent vector size\n",
    "local input = torch.Tensor(config.batchSize, config.n_map_all, config.win_size, config.win_size) # Initialize input tensor\n",
    "local condition = torch.Tensor(config.batchSize, config.n_condition, config.lr_win_size, config.lr_win_size) # Initialize condition tensor\n",
    "\n",
    "print(nz) # Print the latent vector size\n",
    "local noise = torch.Tensor(config.batchSize, nz, 1, 1) # Initialize noise tensor\n",
    "local label = torch.Tensor(config.batchSize) # Initialize label tensor\n",
    "local encode = torch.Tensor(config.batchSize, config.nt_input, 1, 1) # Initialize encoding tensor\n",
    "local seg_target = torch.Tensor(config.batchSize, config.win_size, config.win_size) # Initialize segmentation target tensor\n",
    "local errD, errG # Initialize error variables\n",
    "cutorch.setDevice(1) # Set GPU device\n",
    "input = input:cuda();  noise = noise:cuda();  label = label:cuda();  condition = condition:cuda();  encode = encode:cuda();  seg_target = seg_target:cuda() # Move tensors to GPU\n",
    "local input_record # Initialize input record tensor\n",
    "\n",
    "local input_wrong = torch.Tensor(config.batchSize, config.n_map_all, config.win_size, config.win_size) # Initialize wrong input tensor\n",
    "local condition_wrong = torch.Tensor(config.batchSize, config.n_condition, config.lr_win_size, config.lr_win_size) # Initialize wrong condition tensor\n",
    "input_wrong = input_wrong:cuda(); condition_wrong = condition_wrong:cuda() # Move tensors to GPU\n",
    "\n",
    "if pcall(require, 'cudnn') then # Check if cudnn is available\n",
    "    require 'cudnn' # Load cudnn library\n",
    "    cudnn.benchmark = true # Enable cudnn benchmarking\n",
    "    cudnn.convert(G, cudnn) # Convert Generator to cudnn\n",
    "    cudnn.convert(D, cudnn) # Convert Discriminator to cudnn\n",
    "end\n",
    "D:cuda(); G:cuda(); criterion:cuda(); cri_seg:cuda() # Move networks and criteria to GPU\n",
    "\n",
    "local parametersD, gradParametersD = D:getParameters() # Get parameters and gradients of Discriminator\n",
    "local parametersG, gradParametersG = G:getParameters() # Get parameters and gradients of Generator\n",
    "\n",
    "local normal_holder = torch.Tensor(config.batchSize, config.n_map_all, config.win_size, config.win_size):cuda() # Initialize normal holder tensor\n",
    "\n",
    "local simple_sample = function() # Function to sample data\n",
    "    local ind = torch.randperm(train_ind:size(1)):narrow(1,1,config.batchSize) # Randomly permute training indices\n",
    "    local ind_wrong = torch.Tensor(config.batchSize) # Initialize wrong indices tensor\n",
    "    for i = 1,config.batchSize do ind_wrong[i] = (ind[i] + math.random(train_ind:size(1)-1) - 1) % train_ind:size(1) + 1; end # Generate wrong indices\n",
    "    for i = 1,config.batchSize do ind[i] = train_ind[ind[i]] end # Assign correct indices\n",
    "    for i = 1,config.batchSize do ind_wrong[i] = train_ind[ind_wrong[i]] end # Assign wrong indices\n",
    "    noise:normal(0,1) # Sample noise from normal distribution\n",
    "\n",
    "    for i = 1,config.batchSize do\n",
    "        local t = b_[{{ind[i]},{1},{},{}}] # Get ground truth data\n",
    "        for j = 1,config.n_map_all do\n",
    "            local u = input[{{i},{j},{},{}}]:zero() # Zero out input tensor\n",
    "            u[t:eq(j%config.n_map_all)] = 1 # Set input tensor based on ground truth\n",
    "            input[{{i},{j},{},{}}] = u\n",
    "        end\n",
    "        condition[{{i},{},{},{}}] = lr[{{ind[i]},{},{},{}}] # Set condition tensor\n",
    "        encode[{{i},{},{},{}}] = text[{{ind[i]},{}}]:view(1, config.nt_input, 1, 1) # Set encoding tensor\n",
    "    end\n",
    "\n",
    "    for i = 1,config.batchSize do\n",
    "        local t = b_[{{ind_wrong[i]},{1},{},{}}] # Get wrong ground truth data\n",
    "        for j = 1,config.n_map_all do\n",
    "            local u = input_wrong[{{i},{j},{},{}}]:zero() # Zero out wrong input tensor\n",
    "            u[t:eq(j%config.n_map_all)] = 1 # Set wrong input tensor based on wrong ground truth\n",
    "            input_wrong[{{i},{j},{},{}}] = u\n",
    "        end\n",
    "        condition_wrong[{{i},{},{},{}}] = lr[{{ind_wrong[i]},{},{},{}}] # Set wrong condition tensor\n",
    "    end\n",
    "\n",
    "    for i = 1,config.batchSize do\n",
    "        seg_target[{{i},{},{}}] = b_[{{ind[i]},{1},{},{}}]:view(1,config.win_size,config.win_size) # Set segmentation target tensor\n",
    "    end\n",
    "    seg_target[seg_target:eq(0)] = config.n_map_all # Adjust segmentation target tensor\n",
    "end\n",
    "\n",
    "local real_label = 1 # Define real label\n",
    "local fake_label = 0 # Define fake label\n",
    "local epoch_tm = torch.Timer() # Initialize epoch timer\n",
    "local tm = torch.Timer() # Initialize timer\n",
    "local data_tm = torch.Timer() # Initialize data timer\n",
    "\n",
    "local errD_real, errD_wrong, errD_fake # Initialize error variables for Discriminator\n",
    "local errSeg # Initialize error variable for segmentation\n",
    "local fDx = function(x) # Function to compute Discriminator loss and gradients\n",
    "   gradParametersD:zero() # Zero out Discriminator gradients\n",
    "\n",
    "   simple_sample() # Sample data\n",
    "   input_record = input:clone() # Clone input tensor for record\n",
    "   label:fill(real_label) # Fill label tensor with real label\n",
    "\n",
    "   local output = D:forward{input, encode, condition} # Forward pass through Discriminator\n",
    "   errD_real = criterion:forward(output, label) # Compute real error\n",
    "   local de_do = criterion:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
